{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea33478e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tkinter\n",
    "from tkinter import *\n",
    "from wordcloud import WordCloud, STOPWORDS\n",
    "from PIL import Image, ImageTk\n",
    "from tkinter import messagebox\n",
    "from tkinter import ttk\n",
    "import praw\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.figure import Figure\n",
    "from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "def reddit_search():\n",
    " tb.delete('1.0',END)\n",
    " flag=0\n",
    " test=''\n",
    " reddit = praw.Reddit(client_id=CLIENT_ID,client_secret=CLIENT_SECRET,user_agent=USER_AGENT)\n",
    " #getting title from input\n",
    " Sub = E1.get()\n",
    "    \n",
    " tb.insert(INSERT,'\\n------------------------------\n",
    "Subreddit-------------------------------------\\n')\n",
    " # get 10 hot posts from the given subreddit\n",
    " hot_posts = reddit.subreddit(Sub).hot(limit=10)\n",
    " try:\n",
    " for post in hot_posts:\n",
    " tb.insert(INSERT,post.title)\n",
    " tb.insert(INSERT,\"\\n\")\n",
    " except TclError:\n",
    " pass \n",
    " posts = []\n",
    " this_subreddit = reddit.subreddit(Sub)\n",
    " for post in this_subreddit.hot(limit=1000):\n",
    " posts.append([post.title, post.score, post.id, post.subreddit, \n",
    "post.url, post.num_comments, post.selftext, post.created])\n",
    " \n",
    " posts = pd.DataFrame(posts,columns=['title', 'score', 'id', \n",
    "'subreddit', 'url', 'num_comments', 'body', 'created'])\n",
    " \n",
    " \n",
    " def get_date(created):\n",
    " return dt.datetime.fromtimestamp(created)\n",
    " \n",
    " \n",
    " try:\n",
    " tb.insert(INSERT,posts)\n",
    " _timestamp = posts[\"created\"].apply(get_date)\n",
    " posts = posts.assign(timestamp = _timestamp)\n",
    " tb.insert(INSERT,posts[['title', 'score','timestamp']])\n",
    " tb.insert(INSERT,\"\\n\")\n",
    " #print(posts[['title', 'score','timestamp']])\n",
    " posts['interaction'] = \n",
    "posts['score'].divide(posts['num_comments'],fill_value=1)\n",
    " tb.insert(INSERT,posts[['title', 'score','interaction']])\n",
    " tb.insert(INSERT,\"\\n\")\n",
    " except TclError:\n",
    " pass\n",
    " # Text numerical analysis\n",
    " prostr = \"\"\n",
    " for post in this_subreddit.hot(limit=1000):\n",
    " prostr += post.title\n",
    " \n",
    " data_dict = {}\n",
    " data_set_split = prostr.split()\n",
    " \n",
    " for i in data_set_split:\n",
    " data_dict.setdefault(i,0)\n",
    " data_dict[i] += 1\n",
    " rejected_words = [Sub,Sub.lower(),Sub.upper()]\n",
    " \n",
    " for i in rejected_words:\n",
    " if i in data_dict:\n",
    " del data_dict[i]\n",
    " stopstopstop = STOPWORDS\n",
    " for i in stopstopstop:\n",
    " if i in data_dict:\n",
    " del data_dict[i]\n",
    " title_items = list(data_dict.items())\n",
    " title_items.sort(key=lambda word:word[1],reverse=True)\n",
    " print(title_items)\n",
    " finalstr=\"\\n---------------Most Important Topics----------------------\n",
    "\\n\"\n",
    " for i in range(10):\n",
    " finalstr+=str(title_items[i])+\"\\n\"\n",
    " #print(title_items[i])\n",
    " tb.insert(INSERT,finalstr)\n",
    " '''\n",
    " tb.insert(INSERT,'\\n--------------------------------\n",
    "Description-----------------------------------\\n')\n",
    "'''\n",
    " tb.insert(INSERT,'\\n--------------------------------\n",
    "Graphs-----------------------------------\\n')\n",
    " figure1 = plt.Figure(figsize=(6,5), dpi=100)\n",
    " ax = figure1.add_subplot(111)\n",
    " line1 = FigureCanvasTkAgg(figure1, top)\n",
    " line1.get_tk_widget().grid(row=3,column=1,columnspan=3)\n",
    " posts.plot(kind=\"line\",x='title',y='num_comments',color='red',ax=ax)\n",
    " posts.plot(kind=\"line\",x='title',y='interaction',color='blue',ax=ax)\n",
    " ax.axes.get_xaxis().set_visible(False)\n",
    " ax.set_title('Timewise Presence Of Subreddit \\''+Sub+'\\'')\n",
    " figure3 = plt.Figure(figsize=(6,5), dpi=100)\n",
    " ax3 = figure3.add_subplot(111)\n",
    " ax3.scatter(posts['score'],posts['num_comments'], color = 'b')\n",
    " scatter3 = FigureCanvasTkAgg(figure3, top) \n",
    " scatter3.get_tk_widget().grid(row=2,column=4,columnspan=1)\n",
    " ax3.legend() \n",
    " ax3.set_xlabel('Score')\n",
    " ax3.set_ylabel('num_comments')\n",
    " ax3.set_title('Behaviour Of Subreddit \\''+Sub+'\\'')\n",
    " df1 = posts[['title', 'score']].groupby('title').sum()\n",
    " figure1 = plt.Figure(figsize=(6,5), dpi=100)\n",
    " ax1 = figure1.add_subplot(111)\n",
    " ax1.axes.get_xaxis().set_visible(False)\n",
    " bar1 = FigureCanvasTkAgg(figure1, top)\n",
    " bar1.get_tk_widget().grid(row=3,column=4,columnspan=1)\n",
    " df1.plot(kind='bar', legend=True, ax=ax1)\n",
    " ax1.set_title('Activity Of Subreddit \\''+Sub+'\\'')\n",
    " df2 = posts[['title', 'num_comments']].groupby('title').sum()\n",
    " figure1 = plt.Figure(figsize=(6,5), dpi=100)\n",
    " ax1 = figure1.add_subplot(111)\n",
    " ax1.axes.get_xaxis().set_visible(False)\n",
    " bar1 = FigureCanvasTkAgg(figure1, top)\n",
    " bar1.get_tk_widget().grid(row=3,column=5,columnspan=1)\n",
    " df2.plot(kind='bar', legend=True, ax=ax1)\n",
    " ax1.set_title('Activity Of Subreddit \\''+Sub+'\\' plot 2')\n",
    " \n",
    " dataset = str(posts.title.values)\n",
    " #for post in posts:\n",
    " # dataset=dataset+post.title.str()\n",
    " #print(dataset)\n",
    " \n",
    " wordCloudDf = pd.DataFrame(posts,columns=['title'])\n",
    " \n",
    " comment_words = ''\n",
    " stopstop = set(STOPWORDS) \n",
    " \n",
    " # iterate through the csv file \n",
    " for val in wordCloudDf.title: \n",
    " \n",
    " # typecaste each val to string \n",
    " val = str(val) \n",
    " \n",
    " # split the value \n",
    " tokens = val.split() \n",
    " \n",
    " # Converts each token into lowercase \n",
    " for i in range(len(tokens)): \n",
    " tokens[i] = tokens[i].lower() \n",
    " \n",
    " comment_words += \" \".join(tokens)+\" \"\n",
    " \n",
    " wordcloud = WordCloud(width = 800, height = 800,background_color \n",
    "='white',stopwords = stopstop, min_font_size = 10).generate(comment_words) \n",
    " wordcloud.to_file(\"wordCloud.png\") \n",
    " \n",
    " image = Image.open(\"wordCloud.png\")\n",
    " image=image.resize((450,400),Image.BICUBIC)\n",
    " photo = ImageTk.PhotoImage(image)\n",
    " label = Label(image=photo)\n",
    " label.image = photo \n",
    " label.grid(row=2,column=5,columnspan=1)\n",
    " '''data_dict = {}\n",
    " data_set_split = dataset.split()\n",
    " for i in data_set_split:\n",
    " data_dict.setdefault(i,0)\n",
    " data_dict[i] += 1\n",
    " title_items = list(data_dict.items())\n",
    " title_items.sort(key=lambda word:word[1],reverse=True)\n",
    " print(title_items)'''\n",
    "top = tkinter.Tk()\n",
    "top.tk.call('encoding', 'system', 'utf-8')\n",
    "top.wm_title(\"Reddit Analysis\")\n",
    "top.attributes(\"-zoomed\", True)\n",
    "L1 = Label(text=\"Subreddit Name\")\n",
    "L1.grid(sticky=\"nsew\",row=1,column=1)\n",
    "E1 = Entry(bd =5)\n",
    "E1.grid(row=1,column=2)\n",
    "#call is made here\n",
    "B = tkinter.Button(text =\"Search\", command = reddit_search)\n",
    "B.grid(row=1,column=3)\n",
    "tb=Text(top)\n",
    "tb.grid(row=2,column=1,columnspan=3)\n",
    "top.mainloop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
